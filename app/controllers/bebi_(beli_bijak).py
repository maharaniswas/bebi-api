# -*- coding: utf-8 -*-
"""BeBi (Beli Bijak)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12mR7eSUKPT_NGCPWF_H3ftug1cLvq39y

# Upload Datasets
"""

from google.colab import drive
drive.mount('/content/drive')

"""import modules"""

import math
import scipy
import numpy as np
import pandas as pd
import seaborn as sns

"""read dataset and convert them to dataframe using pandas"""

# instacart datasets
df1 = pd.read_csv('/content/drive/MyDrive/New folder/products.csv')
df2 = pd.read_csv('/content/drive/MyDrive/New folder/aisles.csv')
df8= pd.read_csv('/content/drive/MyDrive/New folder/departments.csv')
df9 = pd.read_csv('/content/drive/MyDrive/New folder/order_products__train.csv')
df10 = pd.read_csv('/content/drive/MyDrive/New folder/orders.csv')

# Amazon datasets
df3 = pd.read_csv('/content/drive/MyDrive/New folder/All Grocery and Gourmet Foods.csv')
df4 = pd.read_csv('/content/drive/MyDrive/New folder/Coffee Tea and Beverages.csv')
df5 = pd.read_csv('/content/drive/MyDrive/New folder/Diet and Nutrition.csv')
df6 = pd.read_csv('/content/drive/MyDrive/New folder/Snack Foods.csv')

# Bigmarket dataset
df7 = pd.read_csv('/content/drive/MyDrive/New folder/BigBasket Products.csv')

"""# Data Exploratory

## Get the shapes of all the dataframes
"""

# content based filtering
print(df3.shape) 
print(df4.shape)
print(df5.shape)
print(df6.shape)
print(df7.shape)

# collaborative filtering
print(df1.shape) # data of products
print(df2.shape) # data of aisle
print(df8.shape) # data of departments
print(df9.shape) # data of orders and products
print(df10.shape) # data of customer orders

"""## Concat dataframes from the same company into 1 dataframe"""

dataset_amazon = pd.concat([df3, df4, df5, df6], axis=0)
dataset_bigmarket = df7

"""## Get the shapes of the joined datasets"""

print(f"Shape of amazon dataset : {dataset_amazon.shape} ")
#print(f"Shape of instacart dataset : {dataset_instacart.shape} ")
print(f"Shape of bigmarket dataset : {dataset_bigmarket.shape} ")

"""## Display the datasets"""

dataset_amazon.head()

dataset_bigmarket.head()

"""## count the null value from each column of each dataset"""

datasetlist = [dataset_amazon, dataset_bigmarket]
for i in range(len(datasetlist)):
  null_value = dict()
  dfnow = datasetlist[i]
  for column in dfnow:
    null_value[column] = dfnow[column].isna().sum()
  print(f"null value of {i+1}th dataset: {null_value}\n")

"""## get the amount of each unique value in the ratings column"""

dataset_bigmarket.rename(columns = {'rating':'ratings', 'sale_price':'discount_price', 'actual_price':'market_price'}, inplace = True)
datasetlist = [dataset_amazon, dataset_bigmarket]
for dataset in datasetlist:
  vals = dict(dataset["ratings"].value_counts())
  print(f"{vals} \n")

"""## get the datatypes of each column"""

# amazon dataset
dataset_amazon.dtypes

# bigmarket dataset
dataset_bigmarket.dtypes

"""# Dataset preprocessing

## Amazon preprocessing

### ratings column
"""

# change the value get to null value
dataset_amazon['ratings'] = dataset_amazon['ratings'].replace(['Get'], np.NaN)

# convert datatype from object to float
dataset_amazon['ratings'] = dataset_amazon['ratings'].astype(str).astype(float)

dataset_amazon['ratings'] = dataset_amazon['ratings'].fillna(0)
dataset_amazon['ratings'].isna().sum()

"""### no_of_ratings column"""

dataset_amazon['no_of_ratings'] = dataset_amazon['no_of_ratings'].replace(',','', regex=True)

vals = dict(dataset_amazon["no_of_ratings"].value_counts())
  print(f"{vals} \n")

# change the value get to null value
dataset_amazon['no_of_ratings'] = dataset_amazon['no_of_ratings'].replace(["FREE Delivery by Amazon"], np.NaN)
dataset_amazon['no_of_ratings'] = dataset_amazon['no_of_ratings'].replace(["Only 1 left in stock."], np.NaN)
dataset_amazon['no_of_ratings'] = dataset_amazon['no_of_ratings'].replace(["Only 2 left in stock."], np.NaN)

dataset_amazon['no_of_ratings'] = dataset_amazon['no_of_ratings'].astype(str).astype(float)

dataset_amazon['no_of_ratings'] = dataset_amazon['no_of_ratings'].fillna(0)
dataset_amazon['ratings'].isna().sum()

"""### price columns"""

dataset_amazon['discount_price'] = dataset_amazon['discount_price'].replace(',','', regex=True)
dataset_amazon['discount_price'] = dataset_amazon['discount_price'].replace('₹','', regex=True)

dataset_amazon['actual_price'] = dataset_amazon['actual_price'].replace(',','', regex=True)
dataset_amazon['actual_price'] = dataset_amazon['actual_price'].replace('₹','', regex=True)

# convert datatype from object to float
dataset_amazon['discount_price'] = dataset_amazon['discount_price'].astype(str).astype(float)
dataset_amazon['actual_price'] = dataset_amazon['actual_price'].astype(str).astype(float)

vals = dict(dataset_amazon['discount_price'].value_counts())
print(f"{vals} \n")

vals = dict(dataset_amazon['actual_price'].value_counts())
print(f"{vals} \n")

dataset_amazon['actual_price'] = dataset_amazon['actual_price'].fillna(0)
dataset_amazon['actual_price'].isna().sum()

dataset_amazon['discount_price'] = dataset_amazon['discount_price'].fillna(dataset_amazon['actual_price'])
dataset_amazon['discount_price'].isna().sum()

"""### Make the train amazon dataset"""

train_amazon = dataset_amazon
train_amazon.drop(train_amazon[train_amazon['actual_price'] <= 0.0].index, inplace = True)
train_amazon.drop(train_amazon[train_amazon['ratings'] == 0.0].index, inplace = True)

train_amazon['main_category'].value_counts()

train_amazon['sub_category'].value_counts()

"""## Bigmarket preprocessing

### description column
"""

dataset_bigmarket['description'] = dataset_bigmarket['description'].astype(str)

dataset_bigmarket['description'] = dataset_bigmarket['description'].fillna('none')
dataset_amazon['ratings'].isna().sum()

"""### drop null value (product without name / brand) and get train data"""

train_bm = dataset_bigmarket.dropna()
train_bm.isna().sum()

train_bm.shape

"""## Transform & Merge Datasets"""

train_bm.rename(columns = {'product':'name', 'category':'main_category', 'market_price':'actual_price'}, inplace = True)

data1 = train_bm[['name','main_category','sub_category','ratings','discount_price','actual_price']]
data2 = train_amazon[['name','main_category','sub_category','ratings','discount_price','actual_price']]
product_dataset = pd.concat([data1, data2], axis=0)

product_dataset.isna().sum()

product_dataset = product_dataset.drop_duplicates()

product_dataset.duplicated().sum()

# Add 1 to the index values
product_dataset.reset_index(inplace=True)

product_dataset['product_id'] = product_dataset.index + 1

product_dataset.shape

# merge instacart data
products = product_dataset[['product_id','actual_price','discount_price','ratings']]
df12 = pd.merge(df2, df1, how='inner', on='aisle_id')
df128 = pd.merge(df12, df8, how='inner', on='department_id')
df1289 = pd.merge(df9, df128, how='inner', on='product_id')
dataset_instacart = pd.merge(df10, df1289, how='inner', on='order_id')

print(products['product_id'].min())
print(products['product_id'].max())

print(dataset_instacart['product_id'].min())
print(dataset_instacart['product_id'].max())

# Drop rows where the value of 'Column2' is greater than 31471
transactions = dataset_instacart.drop(dataset_instacart[dataset_instacart['product_id'] > 22032].index)

print(transactions['product_id'].min())
print(transactions['product_id'].max())

merged_data = pd.merge(transactions, products, how='inner', on='product_id')

merged_data = merged_data.sample(frac=1)
merged_data.head()

# drop irrelevant columns
merged_data.rename(columns = {'product_name':'name', 'department':'category','department_id':'category_id'}, inplace = True)
columns = ['user_id','product_id','name','category_id','category','add_to_cart_order','reordered','actual_price','discount_price','ratings']
df = merged_data
for column in merged_data.columns:
  if column not in columns:
    df = df.drop(column, axis=1)

df.shape

# download the data to google drive
filename = 'final_dataset.csv'

df.to_csv('/content/drive/MyDrive/' + filename)

productset = df[['product_id','name','category_id','category','actual_price','discount_price']]
productset = productset.drop_duplicates(subset='product_id', keep='first')
productset = productset.sort_values('product_id')
filename = 'final_product.csv'
productset.to_csv('/content/drive/MyDrive/' + filename)

"""# ML Model Development (Data Training & Evaluate Model)

## import modules
"""

!pip install tensorflow

!pip install scipy

!pip install pyspark

from sklearn.preprocessing import LabelEncoder, MinMaxScaler # rescale and encode categorical data
from sklearn.model_selection import train_test_split # split data
# module to make neural network model for content based filtering
import tensorflow as tf
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError
from sklearn.metrics import mean_squared_error, mean_absolute_error # evaluate model 
# module to create collaborative filtering recommender system
from sklearn.utils import shuffle
from random import choices
#from scipy.sparse import csr_matrix
#from sklearn.decomposition import TruncatedSVD
from pyspark.sql import SparkSession
from pyspark.ml.recommendation import ALS
from pyspark.sql.functions import sum as spark_sum
from pyspark.sql.types import IntegerType

"""## Load Data"""

# Load the clean dataset
df = pd.read_csv('/content/drive/MyDrive/final_dataset.csv')

df = df.drop(["Unnamed: 0"],axis=1)

"""## Collaborative filtering """

# Step 3: Collaborative Filtering (using PySpark) (same as before)
# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# Create a DataFrame from the preprocessed dataset
spark_df = spark.createDataFrame(df)

# Build a user-item matrix
user_item_matrix = spark_df.groupby("user_id", "product_id").count()

# Split the data into training and testing sets
(training, testing) = user_item_matrix.randomSplit([0.8, 0.2])

# Train a collaborative filtering model using ALS algorithm
als = ALS(maxIter=5, regParam=0.01, userCol="user_id", itemCol="product_id", ratingCol="count",
          coldStartStrategy="drop")
clfmodel = als.fit(training)

# Make personalized recommendations for each user
userRecs = clfmodel.recommendForAllUsers(10)  # Get top 50 recommendations for each user

"""**GET PREDICTION**"""

# Specify the user ID for which you want recommendations
from pyspark.sql.functions import col
user_id = 123

# Get recommendations for the specified user
#user_recommendations = userRecs.filter(userRecs.user_id == user_id).select("recommendations").collect()[0][0]
user_recommendations = userRecs.filter(col("user_id") == user_id).select("recommendations").collect()[0][0]
# Print the recommended items
for recommendation in user_recommendations:
    item_id = recommendation["product_id"]
    predicted_rating = recommendation["rating"]
    print(f"Item ID: {item_id}, Predicted Rating: {predicted_rating}")

"""**GET PREDICTION**"""

from pyspark.sql import Row

# Create a DataFrame with a single row containing the user and product IDs
user_id = 123  # Replace with the actual user ID
product_id = 10714  # Replace with the actual product ID
data = [Row(user_id=user_id, product_id=product_id)]

# Convert the data to a DataFrame
dfs = spark.createDataFrame(data)

# Get the predicted ratings for the product
predicted_ratings = clfmodel.transform(dfs)

# Show the predicted ratings
predicted_ratings.show()

"""**MODEL EVALUATION**"""

from pyspark.ml.evaluation import RegressionEvaluator

# Evaluate the model on the testing set
evaluator = RegressionEvaluator(metricName="rmse", labelCol="count", predictionCol="prediction")
predictions = clfmodel.transform(testing)
rmse = evaluator.evaluate(predictions)

# Print the root mean squared error (RMSE)
print("Root Mean Squared Error (RMSE):", rmse)

"""## Content based filtering"""

# Split the data into training and testing sets (if not already done)
# Preprocessed features and target variable

# Scale numerical features
dataset = df.copy()
scaler = MinMaxScaler()
dataset[["actual_price", "discount_price", "ratings", "add_to_cart_order"]] = scaler.fit_transform(dataset[["actual_price", "discount_price", "ratings", "add_to_cart_order"]])

# Split the dataset into train and test sets
train, test = train_test_split(dataset, test_size=0.2, random_state=42)

features = ["category_id", "discount_price", "reordered", "add_to_cart_order"]
x = train[features]
target = train["ratings"]

"""### Neural network"""

# Define input layers
#user_input = Input(shape=(1,))
category_input = Input(shape=(1,))
other_features_input = Input(shape=(3,))

# Embedding layers for user ID and category ID
#user_embedding = Embedding(input_dim=122010, output_dim=32)(user_input)
category_embedding = Embedding(input_dim=21, output_dim=32)(category_input)

# Flatten the embeddings
#user_flatten = Flatten()(user_embedding)
category_flatten = Flatten()(category_embedding)

# Concatenate the embeddings and other features
concatenated = Concatenate()([category_flatten, other_features_input])

# Dense layers
x = Dense(64, activation='relu')(concatenated)
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)

# Output layer
output = Dense(1, activation='linear')(x)

# Create the model
cbfmodel = Model(inputs=[category_input, other_features_input], outputs=output)

# Compile and train the model
cbfmodel.compile(optimizer='adam', loss='mean_squared_error',metrics=["accuracy"])
cbfmodel.fit([train['category_id'], train[["discount_price", "reordered", "add_to_cart_order"]]], target, epochs=10,batch_size=32)

"""**GET PREDICTION**"""

# Specify the user ID for which you want to make recommendations
target_user_id = 123
# Predict the ratings for all products in the dataset
all_products_data = dataset[dataset['user_id'] != target_user_id]
X_all = [all_products_data['category_id'], all_products_data[["discount_price", "reordered", "add_to_cart_order"]]]
predicted_ratings = cbfmodel.predict(X_all).flatten()


# Create a list of tuples containing product ID and predicted rating
product_ratings = list(zip(all_products_data['product_id'], predicted_ratings))

# Sort the product ratings by predicted rating in descending order
product_ratings.sort(key=lambda x: x[1], reverse=True)

# Get the top 10 recommendations without duplicate product IDs
recommended_products = []
recommended_product_ids = set()

for product_id, _ in product_ratings:
    if product_id not in recommended_product_ids:
        recommended_products.append((product_id, _))
        recommended_product_ids.add(product_id)
    if len(recommended_products) >= 10:
        break

# Filter the original dataset based on the recommended product IDs
top_10_recommendations = all_products_data[
    all_products_data['product_id'].isin([p[0] for p in recommended_products])
]

# Remove duplicate product IDs from the recommendations
top_10_recommendations = top_10_recommendations.drop_duplicates(subset='product_id')

# Keep the top 10 recommendations (if there are more than 10 after removing duplicates)
top_10_recommendations = top_10_recommendations.head(10)
# Print the top 10 product recommendations
print(top_10_recommendations[['product_id', 'name','discount_price','category']])

"""**TEST/EVALUATEE MODEL**"""

# test dataset
X_test = [test['category_id'], test[["discount_price", "reordered", "add_to_cart_order"]]]
predictions = cbfmodel.predict(X_test)

# Assuming you have already trained and obtained predictions from your model
y_true = test['ratings']  # True labels
y_pred = predictions  # Predicted labels

mse = mean_squared_error(y_true, y_pred)

# Calculate mean absolute error (MAE)
mae = mean_absolute_error(y_true, y_pred)

# Print the evaluation metrics
print("Mean Squared Error (MSE):", mse)
print("Mean Absolute Error (MAE):", mae)

"""# Combine Recommender System Model (**GET PREDICTION**)

## get the info from collaborative filtering model
"""

clfrecs = pd.DataFrame(user_recommendations)
clfrecs.columns =['product_id','clf_ratings']
clfinfo = pd.merge(clfrecs,df,on='product_id') 
clfinfo = clfinfo.drop_duplicates(subset='product_id', keep='first')
clfinfo = clfinfo.drop(['user_id','add_to_cart_order','reordered',
                        'category_id','actual_price','ratings'],axis=1)

# Identify the index of the column to move
column_index = clfinfo.columns.get_loc('clf_ratings')

# Create a list of column names in the desired order
new_columns = list(clfinfo.columns[:column_index]) + list(clfinfo.columns[column_index+1:]) + ['clf_ratings']

# Reorder the columns in the DataFrame
clfinfo = clfinfo[new_columns]

"""## get the info from content based filtering model"""

pred_rating = pd.DataFrame(product_ratings)
pred_rating.columns =['product_id','cbf_ratings']
pred_rating = pred_rating.drop_duplicates(subset='product_id', keep='first')
pred_rating['cbf_ratings'] = pred_rating['cbf_ratings'] * (df['ratings'].max() - df['ratings'].min() ) + df['ratings'].min()
pred_rating['cbf_ratings'] = pred_rating['cbf_ratings'].clip(lower=0.0, upper=5.0)
cbfrecs = top_10_recommendations[['product_id', 'name','discount_price','category']]
cbfinfo = pd.merge(cbfrecs, pred_rating, on='product_id')
cbfinfo = cbfinfo.drop_duplicates(subset='product_id', keep='first')
cbfinfo['discount_price'] = cbfinfo['discount_price'] * (df['discount_price'].max()- df['discount_price'].min() ) + df['discount_price'].min()

"""## get predicted ratings from content based filtering on top 10 collaborative filtering recommended items """

finalclf =  pd.merge(clfinfo, pred_rating, on='product_id', how='left')
finalclf

"""## get predicted ratings from collaborative filtering on top 10 content based filtering recommended items """

from pyspark.sql import Row

# Create a DataFrame with a single row containing the user and product IDs
user_id = 123  # Replace with the actual user ID
product_ids = [x for x in cbfinfo['product_id']]  # Replace with the actual product ID
data = [Row(user_id=user_id, product_id=product_id) for product_id in product_ids]

# Convert the data to a DataFrame
dfs = spark.createDataFrame(data)

# Get the predicted ratings for the product
predicted_ratings = clfmodel.transform(dfs)

# Collect the predicted ratings DataFrame into the driver node and convert to Pandas DataFrame
predicted_ratings_pd = predicted_ratings.toPandas()

# Show the predicted ratings
cbf_prod_pred = predicted_ratings_pd[['product_id','prediction']]

finalcbf =  pd.merge(cbfinfo, cbf_prod_pred, on='product_id', how='left')
finalcbf.rename(columns = {'prediction':'clf_ratings'}, inplace = True)
finalcbf

"""## Combine final dataset"""

combined = pd.concat([finalclf, finalcbf], axis=0)
combined

"""## Make the hybrid recommender system"""

# Hybrid Recommender System
def hybrid_recommend(user_id, budget,dataset):

    # Calculate the average rating for collaborative recommendations
    collaborative_avg_rating = np.mean(dataset['clf_ratings'])

    # Calculate the average rating for content-based recommendations
    content_based_avg_rating = np.mean(dataset['cbf_ratings'])

    # Calculate the final recommendations based on weighted average ratings
    dataset['weighted_avg'] = (dataset['clf_ratings'] * collaborative_avg_rating + dataset['cbf_ratings']* content_based_avg_rating) / 2

    hybrid_recommendations = dataset[['product_id','weighted_avg','discount_price']].values.tolist()

    # Sort the recommendations by rating in descending order
    hybrid_recommendations.sort(key=lambda x: x[1], reverse=True)

    # Filter the recommendations based on the user's budget
    filtered_recommendations = []
    total_cost = 0
    for product, rating,price in hybrid_recommendations:
        product_cost = price
        if total_cost + product_cost <= budget:
            filtered_recommendations.append(product)
            total_cost += product_cost
        if len(filtered_recommendations) >= 10:
            break

    return filtered_recommendations

# Example usage
user_id = 123
budget = 10000
recommendations = hybrid_recommend(user_id, budget,combined)
print(recommendations)

"""# Model Deployment

## save and load cbfmodel as .h5 file
"""

#download pickle (h5) file
model = cbfmodel

model.save('cbf_model.h5')

from tensorflow.keras.models import load_model

# Load the model from the .h5 file
loaded_cbf_model = load_model('cbf_model.h5')

"""## save and load clfmodel as ALSModel"""

model1 = clfmodel

model1.save('clmodelnew')

# 2nd option
from pyspark.ml.recommendation import ALSModel
loaded_clf_model = ALSModel.load('/content/clfmodelnew')

"""## deploy model"""

from flask import Flask, request, jsonify
import pandas as pd

app = Flask(__name__)


#load the dataset
df = pd.read_csv('/content/drive/MyDrive/final_dataset.csv')
df = df.drop(["Unnamed: 0"],axis=1)

dataset = df.copy()
scaler = MinMaxScaler()
dataset[["actual_price", "discount_price", "ratings", "add_to_cart_order"]] = scaler.fit_transform(dataset[["actual_price", "discount_price", "ratings", "add_to_cart_order"]])

# get recommendation from collaborative filtering
def collaborative_filtering_model(df, user_id,clfmodel):
  # Get top 10 recommendations for all users (personalized for each user)
  usersRecs = clfmodel.recommendForAllUsers(10)

  # Get recommendations for a specified user
  user_recommendations = usersRecs.filter(usersRecs.user_id == user_id).select("recommendations").collect()[0][0]

  # reformat the item recommendations for user
  clfrecs = pd.DataFrame(user_recommendations) # convert recommendations to a pd dataframe
  clfrecs.columns =['product_id','pred_ratings'] # give column names to dataframe
  clfinfo = pd.merge(clfrecs,df,on='product_id')   # merge with df to get product info
  clfinfo = clfinfo.drop_duplicates(subset='product_id', keep='first') # drop rows with same product id
  # drop irrelevant columns
  clfinfo = clfinfo.drop(['user_id','add_to_cart_order','reordered', 
                          'category_id','actual_price','ratings'],axis=1)

  # Move columns to the correct order
  column_index = clfinfo.columns.get_loc('pred_ratings')
  new_columns = list(clfinfo.columns[:column_index]) + list(clfinfo.columns[column_index+1:]) + ['pred_ratings']
  clfinfo = clfinfo[new_columns]

  return clfinfo

# get recommendations from content based ratings
def content_based_model(df, dataset, user_id, cbfmodel):
  # Predict the ratings for all products in the dataset
  all_products_data = dataset[dataset['user_id'] != user_id]
  X_all = [all_products_data['category_id'], all_products_data[["discount_price", "reordered", "add_to_cart_order"]]]
  predicted_ratings = cbfmodel.predict(X_all).flatten()


  # Create a list of tuples containing product ID and predicted rating
  product_ratings = list(zip(all_products_data['product_id'], predicted_ratings))

  # Sort the product ratings by predicted rating in descending order
  product_ratings.sort(key=lambda x: x[1], reverse=True)

  # Get the top 10 recommendations without duplicate product IDs
  recommended_products = []
  recommended_product_ids = set()

  for product_id, _ in product_ratings:
      if product_id not in recommended_product_ids:
          recommended_products.append((product_id, _))
          recommended_product_ids.add(product_id)
      if len(recommended_products) >= 10:
          break

  # Filter the original dataset based on the recommended product IDs
  top_10_recommendations = all_products_data[all_products_data['product_id'].isin([p[0] for p in recommended_products])]

  # Remove duplicate product IDs from the recommendations
  top_10_recommendations = top_10_recommendations.drop_duplicates(subset='product_id')

  # Keep the top 10 recommendations (if there are more than 10 after removing duplicates)
  top_10_recommendations = top_10_recommendations.head(10)

  pred_rating = pd.DataFrame(product_ratings)
  pred_rating.columns =['product_id','cbf_ratings']
  pred_rating = pred_rating.drop_duplicates(subset='product_id', keep='first')
  pred_rating['cbf_ratings'] = pred_rating['cbf_ratings'] * (df['ratings'].max() - df['ratings'].min() ) + df['ratings'].min()
  pred_rating['cbf_ratings'] = pred_rating['cbf_ratings'].clip(lower=0.0, upper=5.0)
  cbfrecs = top_10_recommendations[['product_id', 'name','discount_price','category']]
  cbfinfo = pd.merge(cbfrecs, pred_rating, on='product_id')
  cbfinfo = cbfinfo.drop_duplicates(subset='product_id', keep='first')
  cbfinfo['discount_price'] = cbfinfo['discount_price'] * (df['discount_price'].max()- df['discount_price'].min() ) + df['discount_price'].min()

  return cbfinfo , pred_rating


# recommendingproducts id
def finaldataset(df,dataset,user_id,cbfmodel,clfmodel):
  # get final collaborative filtering top 10 result data
  clfinfo = collaborative_filtering_model(df, user_id,clfmodel)
  cbfinfo , pred_rating = content_based_model(df, dataset, user_id, cbfmodel)
  finalclf =  pd.merge(clfinfo, pred_rating, on='product_id', how='left')

  # get final content-based filtering top 10 result data
  product_ids = [x for x in cbfinfo['product_id']] # product id of top 10 items
  data = [Row(user_id=user_id, product_id=product_id) for product_id in product_ids]
  dfs = spark.createDataFrame(data)
  predicted_ratings = clfmodel.transform(dfs)
  predicted_ratings_pd = predicted_ratings.toPandas()
  cbf_prod_pred = predicted_ratings_pd[['product_id','prediction']]
  finalcbf =  pd.merge(cbfinfo, cbf_prod_pred, on='product_id', how='left')
  finalcbf.rename(columns = {'prediction':'clf_ratings'}, inplace = True)

  # combine the result
  combined = pd.concat([finalclf, finalcbf], axis=0)

  return combined

def hybrid_recommend(df,combined,user_id,cbfmodel,clfmodel,budget):
    all_recs = finaldataset(df,combined,user_id,cbfmodel,clfmodel)

    # Calculate the average predicted rating for both recommendations
    collaborative_avg_rating = np.mean(all_recs['clf_ratings'])
    content_based_avg_rating = np.mean(all_recs['cbf_ratings'])

    # Calculate the final recommendations based on weighted average ratings

    all_recs['weighted_avg'] = (all_recs['clf_ratings'] * collaborative_avg_rating + all_recs['cbf_ratings']* content_based_avg_rating) / 2
    hybrid_recommendations = all_recs[['product_id','weighted_avg','discount_price']].values.tolist()

    # Sort the recommendations by rating in descending order
    hybrid_recommendations.sort(key=lambda x: x[1], reverse=True)

    # Filter the recommendations based on the user's budget
    filtered_recommendations = []
    total_cost = 0
    for product, rating,price in hybrid_recommendations:
        product_cost = price
        if total_cost + product_cost <= budget:
            filtered_recommendations.append(product)
            total_cost += product_cost
        if len(filtered_recommendations) >= 10:
            break

    return filtered_recommendations

# Define the route for hybrid_recommendation
@app.route('/recommendation', methods=['POST'])
def recommendation():
    data = request.json
    user_id = data['user_id']
    budget = data['budget']
    recommendations = hybrid_recommend(df,combined,user_id,loaded_cbf_model,loaded_clf_model,budget)
    return jsonify(recommendations)

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)